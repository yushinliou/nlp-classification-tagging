# -*- coding: utf-8 -*-
"""slot_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lFZJUzTiANcAUYQglprFqLAjwtumbArk
"""



# Commented out IPython magic to ensure Python compatibility.
# !pwd
# %cd /content/drive/MyDrive/b08305056

import os
os.system('pip install -r requirements.in')

import json
import logging
from argparse import ArgumentParser, Namespace
from collections import Counter
from pathlib import Path
from random import seed
import torch.nn as nn

from preprocess_intent import build_vocab

import torch.optim as optim
import torch.nn.functional as F

import json
import pickle
from argparse import ArgumentParser, Namespace
from pathlib import Path
from typing import Dict

import torch
from torch.optim import Adam
from torch.utils.data import DataLoader
from tqdm import tqdm, trange

# from dataset import SeqTaggingClsDataset
# from model import SeqTagger
from utils import Vocab
import numpy as np

from typing import Dict

import torch
from torch.nn import Embedding

import pandas as pd

import json
import pickle
from argparse import ArgumentParser, Namespace
from pathlib import Path
from typing import Dict

import torch
from tqdm import trange

from utils import Vocab
from torch.utils.data import DataLoader
import logging

from torch import nn
import numpy as np

# path

"""# dataset"""

from typing import List, Dict

from torch.utils.data import Dataset

from utils import Vocab
import torch

is_cuda = torch.cuda.is_available()
if is_cuda:
    device = torch.device("cuda")
    print("GPU is available")
else:
    device = torch.device("cpu")
    print("GPU not available, CPU used")

class SeqTaggingClsDataset(Dataset):
    def __init__(
        self,
        data: List[Dict],
        vocab: Vocab,
        label_mapping: Dict[str, int],
        max_len: int,
    ):
        self.data = data
        self.vocab = vocab
        self.label_mapping = label_mapping
        self._idx2label = {idx: intent for intent, idx in self.label_mapping.items()}
        self.max_len = max_len

    def __len__(self) -> int:
        return len(self.data)

    def __getitem__(self, index) -> Dict:
        instance = self.data[index]
        return instance

    @property
    def num_classes(self) -> int:
        return len(self.label_mapping)

    def label2idx(self, label: str):
        return self.label_mapping[label]

    def idx2label(self, idx: int):
        return self._idx2label[idx]

"""# model"""

class LSTMTagger(torch.nn.Module):

    def __init__(self, embedding_dim, lstm2hidden_dim, hidden_dim,
                 vocab_size, tagset_size, embeddings, num_layers):#, bidirectional):
        ''' Initialize the layers of this model.'''
        super(LSTMTagger, self).__init__()
        
        self.hidden_dim = hidden_dim

        # embedding layer that turns words into a vector of a specified size
        self.word_embeddings = Embedding.from_pretrained(embeddings, freeze=False)
        # the LSTM takes embedded word vectors (of a specified size) as inputs 
        # and outputs hidden states of size hidden_dim
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers)#, batch_first=True)#, bidirectional=bidirectional)

        self.lstm2hidden = nn.Linear(hidden_dim, lstm2hidden_dim)

        self.lstm2hidden1 = nn.Linear(lstm2hidden_dim, lstm2hidden_dim)

        # the linear layer that maps the hidden state output dimension 
        # to the number of tags we want as output, tagset_size (in this case this is 3 tags)
        self.hidden2tag = nn.Linear(lstm2hidden_dim, tagset_size)
        
        # initialize the hidden state (see code below)
        self.hidden = self.init_hidden()

        
    def init_hidden(self):
        # The axes dimensions are (n_layers, batch_size, hidden_dim)
        return (torch.zeros(num_layers, 1, self.hidden_dim).to(device),
                torch.zeros(num_layers, 1, self.hidden_dim).to(device))

    def forward(self, sentence):
        ''' Define the feedforward behavior of the model.'''
        # create embedded word vectors for each word in a sentence
        embeds = self.word_embeddings(sentence)
        # get the output and hidden state by passing the lstm over our word embeddings
        # the lstm takes in our embeddings and hiddent state
        lstm_out, self.hidden = self.lstm(
            embeds.view(len(sentence), 1, -1), self.hidden)
        hidden = self.lstm2hidden(lstm_out.view(len(sentence), -1))
        hidden1 = self.lstm2hidden1(hidden.view(len(sentence), -1))
        tag_outputs = self.hidden2tag(hidden1.view(len(sentence), -1))
        tag_scores = F.log_softmax(tag_outputs, dim=1)
        return tag_scores



def listid2tag(target_list):
  id2tag_list = []
  for targat in target_list:
    targat = [dataset.idx2label(l) for l in targat]
    id2tag_list.append(targat)
  return id2tag_list

def flatten(l):
    return [item for sublist in l for item in sublist]

"""# test"""

def parse_args() -> Namespace:
    parser = ArgumentParser()
    parser.add_argument(
        "test_file",
        type=Path,
        help="Path to the test file.",
        # required=True,
        # default="slot_test.json",
    )

    parser.add_argument(
        "pred_file",
        type=Path,
        help="Path to the pred file.",
        default="pred_slot.csv",
        # required=True,
    )

    parser.add_argument(
        "--cache_dir",
        type=Path,
        help="Directory to the preprocessed caches.",
        default="./cache/slot/",
    )
    parser.add_argument(
        "--ckpt_dir",
        type=Path,
        help="Path to model checkpoint.",
        # required=True,
        default="./ckpt/slot/",
    )


    # data
    parser.add_argument("--max_len", type=int, default=128)

    # model
    parser.add_argument("--hidden_size", type=int, default=512)
    parser.add_argument("--num_layers", type=int, default=2)
    parser.add_argument("--dropout", type=float, default=0.1)
    parser.add_argument("--bidirectional", type=bool, default=True)

    # data loader
    parser.add_argument("--batch_size", type=int, default=128)

    parser.add_argument(
        "--device", type=torch.device, help="cpu, cuda, cuda:0, cuda:1", default="cpu"
    )
    # args = parser.parse_args()
    args, unknown = parser.parse_known_args()
    return args
args = parse_args()

# 讀取詞彙庫
with open(args.cache_dir / "vocab.pkl", "rb") as f:
    vocab: Vocab = pickle.load(f)
    

# 讀取tag和Idx的編號對應
tag_idx_path = args.cache_dir / "tag2idx.json"
tag2idx: Dict[str, int] = json.loads(tag_idx_path.read_text())

# 讀取資料，每筆包含 text,intent,id
data_paths = args.test_file # 路徑
data = json.loads(data_paths.read_text())
dataset = SeqTaggingClsDataset(data, vocab, tag2idx, args.max_len)

# todo
def collate_batch(batch):
    id_list, tokens_list, offsets = [], [], []
    tokens_pipeline = lambda x: [vocab.token_to_id(token) for token in x]
    #tags_pipeline = lambda x: [tag2idx[tag] for tag in x]

    for instance in batch: 
      id_list.append(instance['id'])
      offsets.append(len(instance['tokens']))
      processed_tokens = torch.tensor(tokens_pipeline(instance['tokens']), dtype=torch.int64)
      tokens_list.append(processed_tokens) # 處理過的句子
    tokens_list = torch.cat(tokens_list)
    return id_list, tokens_list.to(device), offsets

test_dataloader = DataLoader(dataset, batch_size=args.batch_size,
                              shuffle=True, collate_fn=collate_batch)

def tokens_pipeline(tokens):
  idx_list = []
  for token in tokens:
    idx_list.append(vocab.token_to_id(token))
  idx_list = torch.tensor(idx_list, dtype=torch.int64).to(device)
  return idx_list

# embedding
embeddings = torch.load(args.cache_dir / "embeddings.pt")

# set up model
# EMBEDDING_DIM = 6
lstm2hidden_dim = 256
emsize = embeddings.shape[1]
HIDDEN_DIM = 512
vocab_size = len(vocab.__dict__['token2idx'])
tagset_size = len(tag2idx)
bidirectional = args.bidirectional
num_layers = 4

# instantiate our model
model = LSTMTagger(emsize, lstm2hidden_dim, HIDDEN_DIM,
                   vocab_size, tagset_size, embeddings, num_layers)#, bidirectional)
print(os.listdir())
model.load_state_dict(torch.load(f'slot.pth'))

model = model.to(device)

idx_list = []
predicted_tag_list = []
model.eval()
for i in range(len(data)):

  token = tokens_pipeline(data[i]['tokens'])
  predicted_tag = model(token)
  tag = predicted_tag.argmax(1).tolist()
  
  idx_list.append(data[i]['id'])
  predicted_tag_list.append(tag)

predicted_tags_label_list = []
for predicted_tag in predicted_tag_list:
  predicted_tag = [dataset.idx2label(tag) for tag in predicted_tag]
  predicted_tags_label_list.append(predicted_tag)

# conver a list of list into a list of string
predicted_tags_label_list = [" ".join(l) for l in predicted_tags_label_list]
# convert list to dataframe
test_df = pd.DataFrame({'id': idx_list,'tags': predicted_tags_label_list})
# exact test number from data, remove "test-" and convert to int for sorting
test_df['num'] = test_df['id'].str.replace("test-", "").astype("int")
# sort value by test number
test_df = test_df.sort_values(by="num")
# remove num column
test_df = test_df[['id', 'tags']]

print(test_df)
test_df.to_csv(args.pred_file, index=False)
print(f'pred_intent save to {args.pred_file}')