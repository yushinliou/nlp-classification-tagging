# -*- coding: utf-8 -*-
"""slot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18JudZUaL37uQK3WxXuvxCil_aJv_ErU-
"""


# Commented out IPython magic to ensure Python compatibility.
# !pwd
# %cd /content/drive/MyDrive/ADL21-HW1-main


import json
import logging
from argparse import ArgumentParser, Namespace
from collections import Counter
from pathlib import Path
from random import seed
import torch.nn as nn

from preprocess_intent import build_vocab

import torch.optim as optim
import torch.nn.functional as F

"""# dataset"""

from typing import List, Dict

from torch.utils.data import Dataset

from utils import Vocab
import torch

is_cuda = torch.cuda.is_available()
if is_cuda:
    device = torch.device("cuda")
    print("GPU is available")
else:
    device = torch.device("cpu")
    print("GPU not available, CPU used")

class SeqTaggingClsDataset(Dataset):
    def __init__(
        self,
        data: List[Dict],
        vocab: Vocab,
        label_mapping: Dict[str, int],
        max_len: int,
    ):
        self.data = data
        self.vocab = vocab
        self.label_mapping = label_mapping
        self._idx2label = {idx: intent for intent, idx in self.label_mapping.items()}
        self.max_len = max_len

    def __len__(self) -> int:
        return len(self.data)

    def __getitem__(self, index) -> Dict:
        instance = self.data[index]
        return instance

    @property
    def num_classes(self) -> int:
        return len(self.label_mapping)

    def label2idx(self, label: str):
        return self.label_mapping[label]

    def idx2label(self, idx: int):
        return self._idx2label[idx]

"""# model"""

from typing import Dict

import torch
from torch.nn import Embedding

class LSTMTagger(torch.nn.Module):

    def __init__(self, embedding_dim, lstm2hidden_dim, hidden_dim,
                 vocab_size, tagset_size, embeddings, num_layers):#, bidirectional):
        ''' Initialize the layers of this model.'''
        super(LSTMTagger, self).__init__()
        
        self.hidden_dim = hidden_dim

        # embedding layer that turns words into a vector of a specified size
        self.word_embeddings = Embedding.from_pretrained(embeddings, freeze=False)
        # the LSTM takes embedded word vectors (of a specified size) as inputs 
        # and outputs hidden states of size hidden_dim
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers)#, batch_first=True)#, bidirectional=bidirectional)

        self.lstm2hidden = nn.Linear(hidden_dim, lstm2hidden_dim)

        self.lstm2hidden1 = nn.Linear(lstm2hidden_dim, lstm2hidden_dim)

        # the linear layer that maps the hidden state output dimension 
        # to the number of tags we want as output, tagset_size (in this case this is 3 tags)
        self.hidden2tag = nn.Linear(lstm2hidden_dim, tagset_size)
        
        # initialize the hidden state (see code below)
        self.hidden = self.init_hidden()

        
    def init_hidden(self):
        # The axes dimensions are (n_layers, batch_size, hidden_dim)
        return (torch.zeros(num_layers, 1, self.hidden_dim).to(device),
                torch.zeros(num_layers, 1, self.hidden_dim).to(device))

    def forward(self, sentence):
        # create embedded word vectors for each word in a sentence
        embeds = self.word_embeddings(sentence)
        # get the output and hidden state by passing the lstm over our word embeddings
        # the lstm takes in our embeddings and hiddent state
        lstm_out, self.hidden = self.lstm(
            embeds.view(len(sentence), 1, -1), self.hidden)
        hidden = self.lstm2hidden(lstm_out.view(len(sentence), -1))
        hidden1 = self.lstm2hidden1(hidden.view(len(sentence), -1))
        tag_outputs = self.hidden2tag(hidden1.view(len(sentence), -1))
        tag_scores = F.log_softmax(tag_outputs, dim=1)
        return tag_scores

"""# define args"""

def parse_args() -> Namespace:
    parser = ArgumentParser()
    parser.add_argument(
        "--data_dir",
        type=Path,
        help="Directory to the dataset.",
        default="./data/slot/",
    )
    parser.add_argument(
        "--cache_dir",
        type=Path,
        help="Directory to the preprocessed caches.",
        default="./cache/slot/",
    )
    parser.add_argument(
        "--ckpt_dir",
        type=Path,
        help="Directory to save the model file.",
        default="./ckpt/slot/",
    )

    # data
    parser.add_argument("--max_len", type=int, default=16)

    # model
    parser.add_argument("--hidden_size", type=int, default=512)
    parser.add_argument("--num_layers", type=int, default=2)
    parser.add_argument("--dropout", type=float, default=0.1)
    parser.add_argument("--bidirectional", type=bool, default=True)

    # optimizer
    parser.add_argument("--lr", type=float, default=1e-3)

    # data loader
    parser.add_argument("--batch_size", type=int, default=128)

    # training
    parser.add_argument(
        "--device", type=torch.device, help="cpu, cuda, cuda:0, cuda:1", default="cpu"
    )
    parser.add_argument("--num_epoch", type=int, default=100)

    # args = parser.parse_args()
    args, unknown = parser.parse_known_args()
    return args

args = parse_args()

"""# read data"""

import json
import pickle
from argparse import ArgumentParser, Namespace
from pathlib import Path
from typing import Dict

import torch
from tqdm import trange

from utils import Vocab
from torch.utils.data import DataLoader
import logging

from torch import nn
import numpy as np

def tokens_pipeline(sentence):
  # sentence = sentence.lower().split()
  idxs = [vocab.token_to_id(word) for word in sentence]
  idxs = np.array(idxs)
  return torch.from_numpy(idxs)
def tags_pipeline(tags):
  idxs = [tag2idx[tag] for tag in tags]
  idxs = np.array(idxs)
  return torch.from_numpy(idxs)

# todo 
def collate_batch(batch):
    tags_list, tokens_list, offset_list = [], [], [0]
    off_sum = 0
    tokens_pipeline = lambda x: [vocab.token_to_id(token) for token in x]
    tags_pipeline = lambda x: [tag2idx[tag] for tag in x]

    for instance in batch:
      off_sum += len(instance['tokens'])
      offset_list.append(off_sum)
      processed_tokens = torch.tensor(tokens_pipeline(instance['tokens']), dtype=torch.int64)
      tokens_list.append(processed_tokens) # 處理過的句子
      processed_tags = torch.tensor(tags_pipeline(instance['tags']), dtype=torch.int64)
      tags_list.append(processed_tags) # 處理過的句子
    offset_list = offset_list[0:-1]
    tokens_list = torch.cat(tokens_list)
    tags_list = torch.cat(tags_list)
    return tags_list.to(device), tokens_list.to(device), offset_list

TRAIN = "train"
DEV = "eval"
SPLITS = [TRAIN, DEV]

args.ckpt_dir.mkdir(parents=True, exist_ok=True)

# 讀取參數
args = parse_args()
args.ckpt_dir.mkdir(parents=True, exist_ok=True)

# 讀取詞彙庫
with open(args.cache_dir / "vocab.pkl", "rb") as f:
    vocab: Vocab = pickle.load(f)
    

# 讀取tag和Idx的編號對應
tag_idx_path = args.cache_dir / "tag2idx.json"
tag2idx: Dict[str, int] = json.loads(tag_idx_path.read_text())

# 讀取資料，每筆包含 text,intent,id
data_paths = {split: args.data_dir / f"{split}.json" for split in SPLITS} # 路徑
data = {split: json.loads(path.read_text()) for split, path in data_paths.items()} # 資料本身
datasets: Dict[str, SeqTaggingClsDataset] = {
    split: SeqTaggingClsDataset(split_data, vocab, tag2idx, args.max_len)
    for split, split_data in data.items()
}

# TODO: init model and move model to target device(cpu / gpu)
is_cuda = torch.cuda.is_available()
if is_cuda:
    args.device = torch.device("cuda")
    print("GPU is available")
else:
    args.device = torch.device("cpu")
    print("GPU not available, CPU used")

# embedding
embeddings = torch.load(args.cache_dir / "embeddings.pt")

# TODO: crecate DataLoader for train / dev datasets
train_dataloader = DataLoader(datasets[TRAIN], batch_size=args.batch_size,
                              shuffle=True, collate_fn=collate_batch)
valid_dataloader = DataLoader(datasets[DEV], batch_size=args.batch_size,
                              shuffle=True, collate_fn=collate_batch)

"""# collate_batch"""

import json
import pickle
from argparse import ArgumentParser, Namespace
from pathlib import Path
from typing import Dict

import torch
from torch.optim import Adam
from torch.utils.data import DataLoader
from tqdm import tqdm, trange

# from dataset import SeqTaggingClsDataset
# from model import SeqTagger
from utils import Vocab
import numpy as np

"""# train"""

# set up model
# EMBEDDING_DIM = 6
lstm2hidden_dim = 256
emsize = embeddings.shape[1]
HIDDEN_DIM = 512
vocab_size = len(vocab.__dict__['token2idx'])
tagset_size = len(tag2idx)
bidirectional = args.bidirectional
num_layers = 4

# instantiate our model
model = LSTMTagger(emsize, lstm2hidden_dim, HIDDEN_DIM,
                   vocab_size, tagset_size, embeddings, num_layers)#, bidirectional)

model = model.to(device)

# define our loss and optimizer
loss_function = nn.NLLLoss()
# optimizer = optim.SGD(model.parameters(), lr=0.3)
optimizer = torch.optim.Adam(model.parameters(),
                             lr=1e-3, betas=(0.9, 0.999), eps=1e-08,
                              weight_decay=0, amsgrad=False)
#criterion = torch.nn.CrossEntropyLoss()

def split_list(offs, target_list):
  split = []
  start = 0
  for i in range(len(offs)-1):
    start = offs[i]
    over = offs[i+1]
    # print(label_test[i][start:over])
    split.append(target_list[start:over])
    # accumulate start point
    start += offs[i]
  return split

def listid2tag(target_list):
  id2tag_list = []
  for targat in target_list:
    targat = [datasets['eval'].idx2label(l) for l in targat]
    id2tag_list.append(targat)
  return id2tag_list

def flatten(l):
    return [item for sublist in l for item in sublist]

def train(dataloader):

    model.train()
    total_acc, total_count = 0, 0
    losses = []
    ture_list, pred_list = [], []

    for idx, (tag, token, offset) in enumerate(dataloader):
        model.zero_grad()
        model.hidden = model.init_hidden()

        # optimizer.zero_grad()
        predicted_label = model(token)
        loss = loss_function(predicted_label, tag)
        # loss = criterion(predicted_label, tag)
        loss.backward()
        losses.append(loss.item())

        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)
        optimizer.step()

        # split list , off計算單句話累計的長度，如 = [0,2,5,7]，按照按照off把predict, tag切開
        predict_list = split_list(offset, predicted_label.argmax(1).tolist())
        tag_list = split_list(offset, tag.tolist())
        # convert id to tag
        tag_list = listid2tag(tag_list) # [[5,5]] -> ["O", "O"]]
        predict_list = listid2tag(predict_list)

        pred_list.append(predict_list) # -> [[["O","O"], ["O", "O"]]]
        ture_list.append(tag_list)

    pred_list = flatten(pred_list) # flatten list -> [["O","O"], ["O", "O"]]
    ture_list = flatten(ture_list)
    # compare predict and true tag, if same add 1
    for pred, true in zip(pred_list, ture_list):
      if (pred == true):
        total_acc += 1
      total_count +=1
        
    return total_acc/total_count, np.mean(losses), pred_list, ture_list

def evaluate(dataloader):
    model.eval()
    total_acc, total_count = 0, 0
    losses, pred_list, ture_list = [], [], []

    with torch.no_grad():
        for idx, (tag, token, offset) in enumerate(dataloader):
            predicted_label = model(token)
            loss = loss_function(predicted_label, tag)
            losses.append(loss.item())

            # split list
            predict_list = split_list(offset, predicted_label.argmax(1).tolist())
            tag_list = split_list(offset, tag.tolist())
            # convert id to tag
            tag_list = listid2tag(tag_list)
            predict_list = listid2tag(predict_list)

            pred_list.append(predict_list)
            ture_list.append(tag_list)
    pred_list = flatten(pred_list)
    ture_list = flatten(ture_list)
    for pred, true in zip(pred_list, ture_list):
      if (pred == true):
        total_acc += 1
      total_count +=1
    return total_acc/total_count, np.mean(losses), pred_list, ture_list

# EPOCHS = args.num_epoch
EPOCHS = 100
epoch_pbar = trange(EPOCHS, desc="Epoch")
epo_list = []
accu_train_list, loss_train_list = [], []
accu_val_list, loss_val_list = [], []

for epoch in range(1, EPOCHS + 1):

    # TODO: Training loop - iterate over train dataloader and update model weights
    accu_train, loss_train, pred_train_list, ture_train_list = train(train_dataloader)
    accu_train_list.append(accu_train)
    loss_train_list.append(loss_train)
    # TODO: Evaluation loop - calculate accuracy and save model weights
    accu_val, loss_val, pred_val_list, ture_val_list = evaluate(valid_dataloader)
    accu_val_list.append(accu_val)
    loss_val_list.append(loss_val)
    # epoch
    epo_list.append(epoch)
    print(f'Epoch {epoch}/{EPOCHS}')
    print('accuracy:{:8.3f} | loss:{:8.3f}'.format(accu_train, loss_train))  
    print('accuracy:{:8.3f} | loss:{:8.3f}'.format(accu_val, loss_val))  
    print('-' * 30)

# save model
model_name = f'{args.ckpt_dir}/slot.pth'
torch.save(model.state_dict(), model_name)

"""# test"""

import pandas as pd

def parse_args() -> Namespace:
    parser = ArgumentParser()
    parser.add_argument(
        "--test_file",
        type=Path,
        help="Path to the test file.",
        # required=True,
        default="./data/slot/test.json",
    )
    parser.add_argument(
        "--cache_dir",
        type=Path,
        help="Directory to the preprocessed caches.",
        default="./cache/slot/",
    )
    parser.add_argument(
        "--ckpt_dir",
        type=Path,
        help="Path to model checkpoint.",
        # required=True,
        default="./ckpt/slot/",
    )
    parser.add_argument("--pred_file", type=Path, default="pred.intent.csv")

    # data
    parser.add_argument("--max_len", type=int, default=128)

    # model
    parser.add_argument("--hidden_size", type=int, default=512)
    parser.add_argument("--num_layers", type=int, default=2)
    parser.add_argument("--dropout", type=float, default=0.1)
    parser.add_argument("--bidirectional", type=bool, default=True)

    # data loader
    parser.add_argument("--batch_size", type=int, default=128)

    parser.add_argument(
        "--device", type=torch.device, help="cpu, cuda, cuda:0, cuda:1", default="cpu"
    )
    # args = parser.parse_args()
    args, unknown = parser.parse_known_args()
    return args
args = parse_args()

# 讀取資料，每筆包含 text,intent,id
data_paths = args.test_file # 路徑
data = json.loads(data_paths.read_text())
dataset = SeqTaggingClsDataset(data, vocab, tag2idx, args.max_len)

# todo 從網路上找來的，要改寫，然後要換到 Dataset.py裡面
def collate_batch(batch):
    id_list, tokens_list, offsets = [], [], []
    tokens_pipeline = lambda x: [vocab.token_to_id(token) for token in x]
    #tags_pipeline = lambda x: [tag2idx[tag] for tag in x]

    for instance in batch: 
      id_list.append(instance['id'])
      offsets.append(len(instance['tokens']))
      processed_tokens = torch.tensor(tokens_pipeline(instance['tokens']), dtype=torch.int64)
      tokens_list.append(processed_tokens) # 處理過的句子
    tokens_list = torch.cat(tokens_list)
    return id_list, tokens_list.to(device), offsets

test_dataloader = DataLoader(dataset, batch_size=args.batch_size,
                              shuffle=True, collate_fn=collate_batch)

def tokens_pipeline(tokens):
  idx_list = []
  for token in tokens:
    idx_list.append(vocab.token_to_id(token))
  idx_list = torch.tensor(idx_list, dtype=torch.int64).to(device)
  return idx_list
tokens_pipeline(['i', 'want', 'it', 'for', '4', 'people', 'at', '10:30am'])

idx_list = []
predicted_tag_list = []
model.eval()
for i in range(len(data)):

  token = tokens_pipeline(data[i]['tokens'])
  predicted_tag = model(token)
  tag = predicted_tag.argmax(1).tolist()
  
  idx_list.append(data[i]['id'])
  predicted_tag_list.append(tag)

predicted_tags_label_list = []
for predicted_tag in predicted_tag_list:
  predicted_tag = [dataset.idx2label(tag) for tag in predicted_tag]
  predicted_tags_label_list.append(predicted_tag)

# conver a list of list into a list of string
predicted_tags_label_list = [" ".join(l) for l in predicted_tags_label_list]
# convert list to dataframe
test_df = pd.DataFrame({'id': idx_list,'tags': predicted_tags_label_list})
# exact test number from data, remove "test-" and convert to int for sorting
test_df['num'] = test_df['id'].str.replace("test-", "").astype("int")
# sort value by test number
test_df = test_df.sort_values(by="num")
# remove num column
test_df = test_df[['id', 'tags']]

test_df.to_csv(f'./predict/pred_slot.csv', index=False)

