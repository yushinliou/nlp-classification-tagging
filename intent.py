# -*- coding: utf-8 -*-
"""intent.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QuyC-yHxygGpYMmrK8M28kKpuVRYyG6j
"""



# Commented out IPython magic to ensure Python compatibility.
# !pwd
# %cd /content/drive/MyDrive/ADL21-hw1-main

import json
import pickle
from argparse import ArgumentParser, Namespace
from pathlib import Path
from typing import Dict

import torch
from tqdm import trange

#from dataset import SeqClsDataset
from utils import Vocab
from torch.utils.data import DataLoader
#import logging

from torch import nn
import numpy as np
from torch.nn import Embedding
import pandas as pd

from typing import List, Dict

from torch.utils.data import Dataset

is_cuda = torch.cuda.is_available()
if is_cuda:
    device = torch.device("cuda")
    print("GPU is available")
else:
    device = torch.device("cpu")
    print("GPU not available, CPU used")


class SeqClsDataset(Dataset):
    def __init__(
        self,
        data: List[Dict],
        vocab: Vocab,
        label_mapping: Dict[str, int],
        max_len: int,
    ):
        self.data = data
        self.vocab = vocab
        self.label_mapping = label_mapping
        self._idx2label = {idx: intent for intent, idx in self.label_mapping.items()}
        self.max_len = max_len

    def __len__(self) -> int:
        return len(self.data)

    def __getitem__(self, index) -> Dict:
        instance = self.data[index]
        return instance

    @property
    def num_classes(self) -> int:
        return len(self.label_mapping)

    def label2idx(self, label: str):
        return self.label_mapping[label]

    def idx2label(self, idx: int):
        return self._idx2label[idx]

TRAIN = "train"
DEV = "eval"
SPLITS = [TRAIN, DEV]

def parse_args() -> Namespace:
    parser = ArgumentParser()
    parser.add_argument(
        "--data_dir",
        type=Path,
        help="Directory to the dataset.",
        default="./data/intent/",
    )
    parser.add_argument(
        "--cache_dir",
        type=Path,
        help="Directory to the preprocessed caches.",
        default="./cache/intent/",
    )
    parser.add_argument(
        "--ckpt_dir",
        type=Path,
        help="Directory to save the model file.",
        default="./ckpt/intent/",
    )

    # data
    parser.add_argument("--max_len", type=int, default=128)

    # model
    parser.add_argument("--hidden_size", type=int, default=512)
    parser.add_argument("--num_layers", type=int, default=2)
    parser.add_argument("--dropout", type=float, default=0.1)
    parser.add_argument("--bidirectional", type=bool, default=True)

    # optimizer
    parser.add_argument("--lr", type=float, default=1e-3)

    # data loader
    parser.add_argument("--batch_size", type=int, default=128)

    # training
    parser.add_argument(
        "--device", type=torch.device, help="cpu, cuda, cuda:0, cuda:1", default="cpu"
    )
    parser.add_argument("--num_epoch", type=int, default=50)

    # args = parser.parse_args()
    args, unknown = parser.parse_known_args()
    return args

class SeqClassifier(nn.Module):

    def __init__(self, embeddings, vocab_size, embed_dim, num_class):
        super(SeqClassifier, self).__init__()
        self.embedding = nn.EmbeddingBag.from_pretrained(embeddings)
        self.fc = nn.Linear(embed_dim, num_class)
        self.init_weights()

    def init_weights(self):
        initrange = 0.5
        self.embedding.weight.data.uniform_(-initrange, initrange)
        self.fc.weight.data.uniform_(-initrange, initrange)
        self.fc.bias.data.zero_()

    def forward(self, text, offsets):

        embedded = self.embedding(text, offsets) # 1016, 300
        out = self.fc(embedded) # 1016, 150
        return out

# 讀取參數
args = parse_args()
args.ckpt_dir.mkdir(parents=True, exist_ok=True)

# 讀取詞彙庫
with open(args.cache_dir / "vocab.pkl", "rb") as f:
    vocab: Vocab = pickle.load(f)

# 讀取intent和Idx的編號對應
intent_idx_path = args.cache_dir / "intent2idx.json"
intent2idx: Dict[str, int] = json.loads(intent_idx_path.read_text())

# 讀取資料，每筆包含 text,intent,id
data_paths = {split: args.data_dir / f"{split}.json" for split in SPLITS} # 路徑
data = {split: json.loads(path.read_text()) for split, path in data_paths.items()} # 資料本身
datasets: Dict[str, SeqClsDataset] = {
    split: SeqClsDataset(split_data, vocab, intent2idx, args.max_len)
    for split, split_data in data.items()
}
# embedding
embeddings = torch.load(args.cache_dir / "embeddings.pt")

# set up model
vocab_size = len(vocab.__dict__['token2idx'])
num_class = datasets['train'].num_classes
embed_dim = embeddings.shape[1]
# embeddings, vocab_size, embed_dim, num_class
model = SeqClassifier(embeddings, vocab_size, embed_dim, num_class).to(args.device)

# TODO: init optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)
criterion = torch.nn.CrossEntropyLoss()

model.parameters()

# define for collate
def text_pipline(text):
  t_list = []
  for t in text.split():
    t_list.append(vocab.token_to_id(t)) # vocab.token_to_id('I') 1
  return t_list
def intent_pipline(intent): # intent2idx['order'] 9
  return intent2idx[intent]

# define collate_fn for dataloader
def collate_fn(batch):
  offset_list, text_list, intent_list = [0], [], []
  off_sum = 0
  for sentence in (batch):
    # text
    tensor_text = torch.tensor(text_pipline(sentence['text']), dtype=torch.int64).to(args.device)
    text_list.append(tensor_text)
    # intent
    intent_list.append(intent_pipline(sentence['intent']))
    # offsets
    off_sum += len(sentence['text'].split())
    offset_list.append(off_sum)

  text_list = torch.cat(text_list)
  intent_list = torch.tensor(intent_list, dtype=torch.int64).to(args.device)
  offset_list = torch.tensor(offset_list[0:-1], dtype=torch.int64).to(args.device)
  return offset_list, text_list, intent_list

# TODO: crecate DataLoader for train / dev datasets
train_dataloader = DataLoader(datasets[TRAIN], batch_size=args.batch_size,
                              shuffle=True, collate_fn=collate_fn)
valid_dataloader = DataLoader(datasets[DEV], batch_size=args.batch_size,
                              shuffle=True, collate_fn=collate_fn)

def train(dataloader):
  model.train() # set model train
  loss_list = []
  acc_ct, acc_total = 0, 0
  for idx, (off, text, intent) in enumerate(dataloader):
    optimizer.zero_grad() # zero optimizer
    predict = model(text, off) # predict
    loss = criterion(predict, intent) # give predict, true intent count loss
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)
    optimizer.step()
    loss_list.append(loss.item())

    acc_ct += (predict.argmax(1) == intent).sum().item()    
    acc_total += intent.size(0) # total text num
    
  return acc_ct/acc_total, np.mean(loss_list)

def evaluate(dataloader):
  model.eval() # set model evaluate
  loss_list = []
  acc_ct, acc_total = 0, 0

  with torch.no_grad():

    for idx, (off, text, intent) in enumerate(dataloader):
      optimizer.zero_grad() # zero optimizer
      predict = model(text, off) # predict
      loss = criterion(predict, intent) # no backward
      optimizer.step()
      loss_list.append(loss.item())

    acc_ct += (predict.argmax(1) == intent).sum().item()        
    acc_total += intent.size(0) # total text num
    
  return acc_ct/acc_total, np.mean(loss_list)

epoch_num = args.num_epoch
epoch_list = []
train_acc_list, train_loss_list = [], []
eval_acc_list, eval_loss_list = [],[]
for epoch in range(epoch_num):
  train_acc, train_loss = train(train_dataloader)
  eval_acc, eval_loss = evaluate(valid_dataloader)
  # append acc
  # print out come
  print(f'Epoch {epoch}/{epoch_num}')
  print('accuracy:{:8.3f} | loss:{:8.3f}'.format(train_acc, train_loss))  
  print('accuracy:{:8.3f} | loss:{:8.3f}'.format(eval_acc, eval_loss))  
  print('-' * 30)

# save model
model_name = f'{args.ckpt_dir}/intent.pth'
torch.save(model.state_dict(), model_name)

"""# test"""

def parse_args() -> Namespace:
    parser = ArgumentParser()
    parser.add_argument(
        "--test_file",
        type=Path,
        help="Path to the test file.",
        # required=True,
        default="./data/intent/test.json",
    )
    parser.add_argument(
        "--cache_dir",
        type=Path,
        help="Directory to the preprocessed caches.",
        default="./cache/intent/",
    )
    parser.add_argument(
        "--ckpt_dir",
        type=Path,
        help="Path to model checkpoint.",
        # required=True,
        default="./ckpt/intent/",
    )
    parser.add_argument("--pred_file", type=Path, default="pred.intent.csv")

    # data
    parser.add_argument("--max_len", type=int, default=128)

    # model
    parser.add_argument("--hidden_size", type=int, default=512)
    parser.add_argument("--num_layers", type=int, default=2)
    parser.add_argument("--dropout", type=float, default=0.1)
    parser.add_argument("--bidirectional", type=bool, default=True)

    # data loader
    parser.add_argument("--batch_size", type=int, default=128)

    parser.add_argument(
        "--device", type=torch.device, help="cpu, cuda, cuda:0, cuda:1", default="cpu"
    )
    # args = parser.parse_args()
    args, unknown = parser.parse_known_args()
    return args
args = parse_args()

# read test data
data = json.loads(args.test_file.read_text())
dataset = SeqClsDataset(data, vocab, intent2idx, args.max_len)

# define collate_fn for dataloader
def collate_fn(batch):
  offset_list, text_list, id_list = [0], [], []
  off_sum = 0
  for sentence in (batch):
    # text
    tensor_text = torch.tensor(text_pipline(sentence['text']), dtype=torch.int64).to(args.device)
    text_list.append(tensor_text)
    # offsets
    off_sum += len(sentence['text'].split())
    offset_list.append(off_sum)
    # id
    id_list.append(sentence['id'])

  text_list = torch.cat(text_list)
  offset_list = torch.tensor(offset_list[0:-1], dtype=torch.int64).to(args.device)
  return offset_list, text_list, id_list

def flat_list(l):
  return [item for sublist in l for item in sublist]

def test(dataloader):
  model.eval() # set model evaluate
  predict_list, id_list = [], []
  acc_ct, acc_total = 0, 0

  with torch.no_grad():

    for idx, (off, text, id) in enumerate(dataloader):
      predict = model(text, off) # predict
      predict_list.append(predict.argmax(1).tolist())
      id_list.append(id)
  predict_list = flat_list(predict_list)
  predict_list = [dataset.idx2label(predict) for predict in predict_list]
  id_list = flat_list(id_list)
  return predict_list, id_list

test_dataloader = DataLoader(dataset, batch_size=args.batch_size,
                            shuffle=True, collate_fn=collate_fn)

def test_df(dataloader):
  predict_list, id_list = test(test_dataloader) # get predict outcome from dataloader
  test_df = pd.DataFrame({'id': id_list, 'intent': predict_list})
  test_df['num'] = test_df['id'].str.replace("test-", "").astype('int')
  test_df = test_df.sort_values(by=['num'])
  test_df = test_df[['id', 'intent']]
  return test_df

test_df = test_df(test_dataloader)

test_df.to_csv('./predict/pred_intent.csv', index=False)